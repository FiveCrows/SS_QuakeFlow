import re
import matplotlib.pyplot as plt
import numpy as np
import statistics as stats
from datetime import datetime, timedelta
from scipy.spatial.transform import Rotation as R
from collections import defaultdict
import json
import obspy
import requests
import os
#import simplekml (moved to gen kml function)
import laspy
import pandas as pd
from pyproj import Transformer 
from collections import defaultdict
from obspy.core.inventory import Inventory, Network, Station, Channel, Response
from obspy.core import UTCDateTime, AttribDict
from obspy.clients.nrl import NRL
from bisect import bisect
from tqdm import tqdm
import utm
api_openTopography = "143a4c09555886ceb9760b6cf432902c"

class SS_inventory(Inventory):
    def __init__(self, dir = "SS_inventory", subdir_fr = "FRtests", subdir_net = "paradox1", results_addendum = '_'):
        """Creates an inventory object for arry of usmartsolo geophones

        Args:
            dir (str, the directory of the inventory object): _description_. Defaults to "SS_inventory".
            fr_subDir (str, the dir subfolder for files from the smartsolo test rack): . Defaults to "SS_frTests".
            net_subDir (str, optional): the dir  subfolder for   data folders pulled from smartsolos. Defaults to "paradox1".
        """
        
        self.dir = "SS_inventory"
        dir_fr=os.path.join(dir,subdir_fr,'ss_full.xlsx')
        dir_net = os.path.join(dir,subdir_net)
        tests = pd.read_excel(dir_fr,'Sensor Properties Test',header = 2, index_col = 'SN')
        networks = [(SS_net(dir_net,tests,self))]
        super().__init__(networks)


    def pickWithPhasenet():    
        pass
    
    def predWithGamma():
        pass

    def exportStationsXML():
        pass
    
    def rePredictHypoDD():
        pass

    def compileTests(self):
        testFiles = list(filter(lambda x: x.endswith(".xlsx"), os.listdir(self.frDir)))
        initial = True
        df = pd.DataFrame()
        for file in testFiles:
            test = pd.read_excel(self.frDir +"/"+file)
            settings= test.iloc[0,0].split(',')
            sample_rate = float(re.findall(r"[-+]?(?:\d*\.*\d+)", settings[0])[0])#'ms'
            gain = int(re.findall(r"[-+]?(?:\d*\.*\d+)", settings[1])[0])#'db'
            columns = list(test.iloc[2,:])
            if 'Linear' in settings[3]:
                aaFilter = "Linear"
            else:
                aaFilter = "Minimum"
            test = test[3:]
            test.columns = columns
            test["sample_rate"]= sample_rate
            test["gain"]= gain
            test["aaFilter"]= aaFilter
            df = pd.concat([df,test])    
        df.to_csv(self.frDir+"/tests.csv" )
        #df[(df[d.keys()] == d.values()).all(axis=1)] line for getting 

        d = {"SN": int(self.station.alternate_code), "sample_rate": (1000/self.sample_rate),"gain": self.gain}# "aaFilter": self.aaFilter, 
        matches = tests[(tests[d.keys()] == d.values()).all(axis=1)]#find test with matching parameters
        if len(matches) == 0:
            print("no match!")
            return None
            raise Exception("Error, no matching test!")     
        match = matches.sort_values("Test Time").iloc[-1]#get latest test
        self.sensitivity = match[self.axes + " S.Sensitivity.(V/m/s)"]
        self.damping = match[self.axes + " S.Damping"]
        self.w0 = match[self.axes + " N.Freq (Hz)"]*2*np.pi
        sensitivity = 1000*self.sensitivity*10**(0.05*self.gain)#adjust with gain, also mV to V  
        self.response = Response.from_paz([0,0],self.getPoles()[1:], self.sensitivity)        
    
    def extendZNE (self):
        new_nets = self.nets.copy()
        self.network.extendTo_ZNE()

class SS_net(Network):
    """Container for an array of SmartSolo Stations, extending the obspy Network class 
    Each    the DigiSolo.Log 
    handles geospatial transforms"""

    reg_system = "epsg:4326"#latitude, longitude
    las_system = "epsg:26912"#what opentopology uses for a metric system in SLC
    transformer = Transformer.from_crs("EPSG:4326", "EPSG:26912")
    #laspy.read("topography/points.laz")

    def __init__(self,dir, fr_tests, inventory, lidar_pc = None, location = "NA"):
        """ loads a network from each subdirectory. Subdirectories must contain 
        details of a respective station on a DigiSolo.LOG with SmartSolos data format.


        Args:
            dir (String): A path to smartsolo directories
            fr_tests (DataFrame): fr data generated by the smartsolo calibration device 
            inventory (Inventory): parent object
            lidar_pc (_type_, optional): _description_. Defaults to None until  updated  lidar processing
            location (str, optional): Something to label the net with. Defaults to "NA".
        """
        if os.path.exists(dir+"/metadata.json"):
            metadata = json.load(open(dir+"/metadata.json"))
            self.recoveryTime = UTCDateTime(metadata["recoveryTime_UTC"])
        self.dir = dir
        #self.pc_dir = dir+"/"+lidar_pc
        self.netName = dir.split('/')[-1]
        self.stations = []
        self.inventory = inventory
        for sub in tqdm(os.listdir(dir)):
            if sub.isnumeric():
                try:
                    test = fr_tests.loc[int(sub)]
                except:
                    test = None
                self.includeStation(dir+"/"+sub,test)
                
        #self.pc = laspy.read(self.pc_dir)
        super().__init__(self.netName[-2:],self.stations, alternate_code = self.netName)
        #box region of array
        #self.miny = min([s.latitude] for s in self.stations) [0]
        #self.maxy = max([s.latitude] for s in self.stations) [0]
        #self.maxx = max([s.longitude] for s in self.stations)[0]
        #self.minx = min([s.longitude] for s in self.stations)[0]


    def lazTolas():
        """
        For lidar processing, just to decompress laz files and saves into las files that load a little faster if wanted
        """
        las = laspy.read(self.pc_dir)
        las = laspy.convert(las)
        las.write(self.dir+"/pc.las")

    def includeStation(self,dir, fr_dir):
        """Constructs a station by a DigiSolo.LOG datafile

        Args:
            dir (_type_): _description_
            fr_dir (_type_): _description_
        """
        station = SS_Station(dir,fr_dir, self)
        self.stations.append(station)
    
    def findTopographies(self,format = "PointCloud"):
        """for lidar processing..."""
        """checks opentopography for available topographical datasets in the deployment region

        Args:
            format (str, optional): _description_. Defaults to "PointCloud".

        Returns:
            _type_: _description_
        """
        url_findTops = "https://portal.opentopography.org/API/otCatalog?productFormat={}&minx={}&miny={}&maxx={}&maxy={}&detail=true&outputFormat=json&include_federated=true".format(format,self.minx,self.miny,self.maxx,self.maxy)        
        response = requests.get(url_findTops).json()
        return response
    
    def genKML(self):
        """
        stores the coordinates of each station in a kml file under dir.
        the kml can be loaded in google earth
        """
        import simplekml
        kml = simplekml.Kml()
        for s in self.stations:
            kml.newpoint(name = "SS_{}".format(s.serial_number), coords = [(s.longitude,s.latitude)])
        kml.save(self.dir+"/GPS.kml")

    def getStation_SN(self, sn):        
        return {s.serial_number:s for s in self.stations}[sn]

    def getDistance(self,s1,s2):
        """
        returns  meters of separation between station objects s1 and s2
        """
        c1 = self.transformer.transform(s1.longitude,s1.latitude)
        c2 = self.transformer.transform(s2.longitude,s2.latitude)
        return c2-c1
    
    def loadStreams_time(self,startTime,endTime):
        """calls loadStream_time for every station in the net
            see net.loadStream_time

        Args:
            startTime (UTC_DateTime): stream start
            endTime (UTC_DateTime): stream end
        """
        
        for station in tqdm(self.stations):
            station.loadStream_time(startTime, endTime)

    def loadStreams_number(self,number):
        """calls loadStream_number for every station in the net

        Args:
            number (int): the stream filenumber
        """
        
        for station in tqdm(self.stations):
            station.loadStream_number(number)

    def genStreams_picks(self, picks, window = 20):
        for station in self:
            station.genStreams_picks(picks,window)
    def get_dt(self, r, theta, depth, v_max,v_min):
        trans = self.transformer.transform
        s_c = self.getStation_SN(453017489)         
        lat_m, long_m = trans(s_c.latitude, s_c.longitude)                    
        lat_m = lat_m +r*np.sin(theta)
        long_m = long_m+np.cos(theta)
        dt = np.array([(lambda p: v_max*np.sqrt(((lat_m-p[0])**2+(long_m-p[1])**2)))(trans(s.latitude, s.longitude))+v_min*depth for s in self])/1000
        return dt
            
    def beam_analysis(self,r_vals, x_vals, theta_vals, v_max,v_min, t_min, t_max):
        theta_vals = np.linspace(0,2*np.pi,4)
        r_vals = np.linspace(0,4,5)
        depths = np.linspace(2,6,3)
        x_vals = np.linspace(-5,5,10)
        t_min = max([s[0].stats.starttime for s in streams])
        t_max = min([s[0].stats.endtime for s in streams])        
        streams = [s.loadStream_time(t_max,t_min+100000).decimate(10) for s in self]
        z = ([s[2].copy() for s in streams])
        [z.trim(t_min,t_min+dt_mit) for z in z]
        [len(s) for s in streams]
        #obspy.signal.array_analysis.array_processing(streams, t_max-t_min, 1, )
        #ans = [[[beamPower(get_dt(self,r,theta, depth,v_max,v_min),z) for depth in depths] for r in r_vals] for theta in theta_vals]

    def beamPower(dt,stream):
        streams_mig = obspy.Stream([stream[i].trim(dt[i], dt[i]-max(dt)-1) for i in range(len(dt))]).stack()
        fft = np.fft.fft(np.mean(streams_mig))
        return np.mean(fft*fft.conjugate())
    
    def write(self,streamGen, addendum = '_'):
        """writes network details with obspy formats in both xml and txt which are  processable by phasenet and gamma, 
        and a csv with providing the storage directory for all streams in streamGen

        Args:
            addendum (str, optional): something distinguish the new folder name with . Defaults to '_'.
        """
        outDir = self.dir+addendum
        fnames = []
        if not os.path.exists(outDir):
            os.mkdir(outDir)                
        for s in tqdm(self.stations):            
            for stream in tqdm(streamGen):
                fnames.append(s.writeStream(stream))#
        xmlDir = outDir+"/stations.xml"
        txtDir = outDir+"/stations.txt"
        #if not os.path.exists(xmlDir):
        self.axisTo_ZNE()
        self.inventory.write(xmlDir,format = "stationxml", validate = True)
        self.inventory.write(txtDir,format = "stationtxt", validate = True)
        fnameCSV = pd.Series([f for f in fnames if f is not None], name = 'fname')        
        fnameCSV.to_csv(self.dir+'_'+'/mseed.csv', index = False)
        
    def plotLogNumericals(self, includeMemory = False):
        refStation = self.stations[0]
        numericSeries = refStation.numericSeries
        if not includeMemory:
            numericSeries = [key for key in numericSeries if refStation.typeKeys[key][0] != "Memory"]
        n = len(numericSeries)        
        factors = [i for i in range(2,n) if n%i == 0]
        if len(factors) == 0:
            n = n+1
            factors = [i for i in range(2,n) if n%i == 0]
        d2 = factors[np.floor(len(factors)/2+0.5).astype(int)]        
        fig,ax = plt.subplots(n//d2,d2)
        for station in self.stations:
            station.plotNumericals((fig,ax),includeMemory = includeMemory)
        handles, labels = plt.gca().get_legend_handles_labels()
        by_label = dict(zip(labels, handles))
        fig.legend(by_label.values(), by_label.keys(),loc='outside upper right', fontsize = 16)        
        fig.suptitle(self.stations[0].start_date + " Numerical Data for multiple stations", fontsize = 30)
        plt.tight_layout()
        plt.savefig(self.stations[0].start_date.replace('/','_')+"_{}_logNumericals.png".format(self.netName))    
        plt.show()

    def plotAgainstTemp(self):
        """
        plots temperature against other numerical station records.   
        """

        refStation = self.stations[6]
        numericSeries = refStation.numericSeries        
        numericSeries = [key for key in numericSeries if refStation.typeKeys[key][0] != "Memory"]
        key = numericSeries.pop()        
        numericSeries.pop()
        numericSeries.pop(2)
        idx, values = zip(*[p for p in refStation.data[key] if p[0] is not None])
        df = pd.DataFrame(values,idx)
        df.columns = ([key])
        for key in numericSeries:
            idx, values = zip(*[p for p in refStation.data[key] if p[0] is not None])
            df2 = pd.DataFrame(values,idx)
            df2.columns = ([key])    
            df = pd.merge_asof(df,df2, right_index = True, left_index = True)
            #df = df.merge(df2, right_index = True, left_index = True)
            if len((df))>0:
                print(df)
        df.set_index('temperature')
        #df.plot()
        return df

    def axisTo_ZNE(self):
        """unfinished"""
        for s in self.stations:
            sZNE = s.axisTo_ZNE()
            
    def export_gammaCSV(self):
        """export station data to a csv format processable with gamma
        unfinished
        """
        keys = {id, longitude, latitude, elevation, components, response, x, y, z}
        #getVal{}
        pass
        
class SS_Station(Station):
    """
    Extension of obspy Station that reads SmartSolo Files during construction,
    and augments itself with XYZ SS_Channels 
    """

    def __init__(self,dir, frTest, network = None, recoveryTime = None):
        """init function

        Args:
            dir (string): A folder containing files from a smartsolo device, 
              should contain a file DigiSolo.LOG, and files seis00**.MiniSeed
            fr_dir (string): A folder containing frequency response files, containing SS test results
            network(SS_Network): The network object with the rest of stations in the array
            recoveryTime(UTCDateTime): When the geophone was taken from retrieved from the ground
        """        

        self.cycleTimes = []        
        self.dir = dir#where to read digisolo log 
        self.network = network#network parent object 
        self.recoveryTime = recoveryTime#time of geophone recovery 
        self.readLog(dir+"/DigiSolo.LOG")#loads data from digisolo.log to self
        #pass device info from log onto object
        super().__init__(str(self.serial_number)[-5:],self.latitude, self.longitude, self.altitude, alternate_code = str(self.serial_number))
        for key in self.typeKeys:
            if self.typeKeys[key][0] == 'DeviceInfo':
                setattr(self,key,self.data[key][-1][1])
        self.start_date = UTCDateTime(self.data["start_acquisition_filename"][0][0])
        self.n_files  = len(self.data['start_acquisition_filename'])+len(self.data['changed_acquisition_filename'])
        print(self.start_date)
        
        #define XYZ orientation from the digisolo files roll pitch north
        r1 = R.from_euler('XYZ', [self.datStats["roll_angle"]["mean"], self.datStats["pitch_angle"]["mean"], self.datStats["ecompass_north"]["mean"]], degrees=True)
        xyz = {'Y':[1,0,0],'X':[0,1,0],'Z':[0,0,1]}
        dips = {key:-90+np.arccos(np.dot(r1.apply(xyz[key]),[0,0,1]))*180/np.pi for key in xyz}
        azimuths = {key:np.arctan2(r1.apply(xyz[key])[0],r1.apply(xyz[key])[1])*180/np.pi for key in xyz}
        gains = {'X':self.channel_1_gain,'Y':self.channel_2_gain,'Z':self.channel_3_gain}
        #make sample rate in hz
        self.sample_rate = 100000/self.sample_rate        
        fileTimes = self.data["changed_acquisition_filename"]+self.data["start_acquisition_filename"]
        fileTimes.sort()
        ### because of case error,"Seis" in log but actually "seis in file"
        times,names = list(zip(*fileTimes))
        names = [s[:1].lower() + s[1:] for s in names]
        fileTimes =list( zip(times,names))
        self.fileTimes = fileTimes
        #for smartsolo stations with three channels  
        for key in xyz.keys():
            channel = SS_Channel(self, frTest, key, dips[key],azimuths[key], gains[key])
            self.channels.append(channel)
        try:
            self.setFR(frTest)
        except:
            print("no response")        

    def readTime(self,line):
       ###to read lines with time in solo logs
       line = re.findall('"([^"]*)"', line)[0]
       return datetime.strptime(line, "%Y/%m/%d,%H:%M:%S")
           
    def readFloat(self,line):
        """ read lines with floats in solo logs
        Args:
            line (string): float string
        Returns:
            float: the float from the string
        """       
        return (re.findall("[-+]?(?:\d*\.*\d+)",line))    
    #read smartsolo log file 

    def readLog(self,logDir):
        """reads smartsolo log files from directory
           and stores the information as a dict that detects
           data blocks and keys,
           parses each key = value pairs a structure like {key:[(block time, value)]           
           typeKeys to store each keys block and data type like {key:(key block, data type)}
           and blockTimes as {blockKey:[block times]}

        Args:
            logDir (string): directory must contain a file DigiSolo.LOG
        """

        with open(logDir) as f:
            f = f.readlines()
        blockHeads = [n for n in range(len(f)) if f[n].startswith("[")]
        data = defaultdict(list)
        typeKeys = {}
        readKeys = {"int":       lambda x: np.nan if not x.isnumeric() else int(x) ,
                    "string":    lambda x: x.replace("\"",""),#remove quotes
                    "doubleList":lambda x: [float(x) for x in s],
                    "double":    lambda x: float(x),
                    "unknown":   lambda x: x
                    }#how to read values for different sorts of keys
        ######build data into organized dicts
        blockTimes = defaultdict(list)
        for i in range(len(blockHeads)):    
            header = re.match(r"([A-za-z]+)", f[blockHeads[i]][1:]  , re.I).group()
            top = blockHeads[i]+1
            blockHeads.append(len(f))
            bot = blockHeads[i+1]    
            splits = [line.split("=")   for line in f[top:bot] if "=" in line]
            splits = {split[0].strip().lower().replace(" ","_").replace("-","_"):split[1].strip() for split in splits}    
            #so time can be seperated as the index 
            if "utc_time" in splits.keys():        
                time = self.readTime(splits.pop("utc_time"))                                    
                blockTimes[header].append(time)
            else:
                time = None
            for (key,value) in splits.items():        
                #identify what sort of value the key pairs to
                if key not in data:        
                    if value.isnumeric():                
                        typeKeys[key] = (header,"int")
                    elif value[0] == "\"":                
                        typeKeys[key] = (header,"string")
                    elif re.match("[-+]?(?:\d*\..*\d+)",value):
                        s = value.split(',')
                        if len(s)>=2:                    
                            typeKeys[key] = (header,"doubleList")
                        else:                    
                            typeKeys[key] = (header,"double")
                    else:
                            typeKeys[key] = (header,"unknown")
                #finally, read and store the value where it belongs
                parsedValue = readKeys[typeKeys[key][1]](value)
                data[key].append((time, parsedValue))
                
        self.data = data
        self.typeKeys = typeKeys
        self.blockTimes = blockTimes
        #angle_floats = ["eCompass North", "Tilted Angle", "Roll Angle", "Pitch Angle"]
        #pos_floats = ["Longitude", "Latitude", "Altitude"]                                
        #check for possible unwanted gap in time 
        for timeSeries in blockTimes.items():
            diff = np.diff(timeSeries[1])
            mode = stats.mode(diff)
            dtMax = np.argmax(diff)
            if diff[dtMax]>mode*3:            
                print("Time gap of {} from".format(diff[dtMax]) +  " detected in {} sn {}".format(timeSeries[0],data["serial_number"][0][1]))    
               
        self.numericSeries = [key for key in typeKeys if len(data[key])>4 and (typeKeys[key][1] == 'int' or typeKeys[key][1] == 'double')]
        stdevs = {}
        datStats = {}
        ##### further process numerical data  #remove outliers, get some stats
        for key in self.numericSeries:
            ####cut out gps data if collected after geophone recovery begun
            times,vals = list(zip(*data[key]))
            if self.recoveryTime == None:
                cut = -1
            else:
                cut = np.searchsorted(times, self.recoveryTime)
            vals = vals[:cut]                        

            q1 = np.nanpercentile(vals, 25)
            q3 = np.nanpercentile(vals, 75)
            IQR = q3-q1
            lwr_bound = q1-(3*IQR)
            upr_bound = q3+(3*IQR)
            for i in range(len(vals)):            
                if vals[i] < lwr_bound or vals[i] > upr_bound:
                    data[key][i] = (data[key][i][0],None)
            stdevs[key] = np.nanstd(vals)
            datStats[key] = {'mean': np.nanmean(vals),
                             'std': stdevs[key],
                             'mean_err': stdevs[key]/np.sqrt(len(np.array(vals)[~np.isnan(vals)]))
                            }
        #with outliers gone save the data to the object
        self.data = data
        ####Get statistics on calculated data                        
        self.datStats = datStats
        self.latitude = self.datStats['latitude']['mean']
        self.altitude = self.datStats['altitude']['mean']
        self.longitude = self.datStats['longitude']['mean']        
        self.serial_number = data["serial_number"][-1][1]
        #convert to meters
        for key in ["latitude","longitude"]:
                datStats[key]['m_err'] = datStats[key]['mean_err']*111139
                datStats['altitude']['m_err'] = datStats['altitude']['mean_err']    

    #def downSample(self,sample_rate):
    #    """compresses station stream with decimation for bulk of downsampling, 
    #    and then perform an exact resampling with a fourier resample,
    #    down to sample_rate, and updates object parameters accordingly 
#
    #    Args:
    #        sample_rate (int): the desired sample_rate, hz
    #    """
    #    st_sr = self.stream[0].stats.sampling_rate
    #    scale = int(np.floor(st_sr/sample_rate))
    #    if scale >16:
    #        scale = 16
    #    remain = st_sr/sample_rate-scale
    #    print("scale{}".format(scale))
    #    self.stream.decimate(scale)
    #    if remain != 0:
    #        self.stream.resample(sample_rate)
    #    self.sample_rate = sample_rate

    def trimTimeseries(startUTC,endUTC):
        pass
    def plotNumericals(self, subplots = None,includeMemory = False, series = None):
        """Plotting tool for numerical time series from ss logs

        Args:
            subplots (_type_, list): pyplot axes list, used to plot whole nets
            includeMemory (bool, optional): whether to include numerical memory 
        """
        ########plot for numerical data        
        #find squarest factors for numberSeries
        # 
        if includeMemory:   
            numericSeries = self.numericSeries
        elif series == None:        
            numericSeries = [key for key in self.numericSeries if self.typeKeys[key][0] != "Memory"]
        else: 
            numericSeries = series

        n = len(numericSeries)
        if subplots == None:
            factors = [n,1]
            if n<6:
                factors = [n,1]
            elif len(factors) == 0:#in case number is prime
                n == n+1
                factors = [i for i in range(2,n) if n%i == 0]
            else:
                factors = [i for i in range(2,n) if n%i == 0]
            d2 = factors[np.floor(len(factors)/2+0.5).astype(int)]
            fig,ax = plt.subplots(d2,n//d2)
        else:
            fig,ax = subplots
            d2=ax.shape[1]

        #plot out with that grid
        for i in range(n):
            series = numericSeries[i]
            (x,y) = list(zip(*self.data[series]))    
            axPick = ax[i%d2, i//d2]
            axPick.plot(x,y,label = self.script_name)
            axPick.set_ylabel(series, fontsize = 16)
            axPick.set_xticklabels(axPick.get_xticklabels(),rotation = 30)
                    
        if subplots == None:
            fig.suptitle("{} Numerical Data for Station {}".format(self.start_date,self.serial_number))
            plt.savefig("{}_{}_logPlot.png".format(self.start_date,self.serial_number).replace('/','_'))    
            plt.tight_layout()                   
            plt.show()
        ########plot for string data
        #textSeries = [key for key in typeKeys if len(dataDict[key])>2 and (typeKeys[key][1] == 'string')]
    
    def loadStream_file(self, fname, dir = None):
        """load Stream by file name, and edit stream metadata to match with station        
        file string may use * to pick up multiple files
        Args:
            fname_start (str) select the files to load the stream for by the starting text.

        Returns: 
            Stream
        """
        if dir == None:
            dir = self.dir
        fdir = dir+"/"+fname 
        print(fdir)  
        stream = obspy.read(fdir)
        for trace in stream:
            id = trace.id.split(".")
            id[0] = self.network.code
            id[1] = self.code
            trace.id = ".".join(id) 
            c = AttribDict({'latitude':self.latitude,
                            'longitude':self.longitude,
                            'elevation':self.altitude})                       
            trace.stats.coordinates = c
            
            
        return stream

    def loadStream_number(self, number):
        """Load SS output Stream by file number

        Args:
            number (int): file number
        Returns:
            None
        """
        nZeros = 3-len(str(number))
        fname = "seis"+"0"*nZeros+"{}*.MiniSeed".format(number)             
        stream = self.loadStream_file(fname)           
        stream.code = number                
        return stream
        
    def findFile_time(self, dateTime, component = '*'):
        """Retrievs file beginning with specified dateTime
        Args:
            dateTime (UTCDateTime): the returned file will contain data for this time
            component (str): the Channel to Select a component for, 'Z','X','Y'. Defaults to '*' to select all components

        Returns:
            _type_: _description_
        """
        
        times,values = zip(*self.fileTimes)
        times = list(times)        
        if dateTime<times[0]:
            print("Time starts before data")
            return None
        file = values[bisect(times,dateTime)-1]        
        file = file.replace('.', component + '.')
        return file 
    
    def loadStream_time(self, startTime, endTime, component = '*'):
        """finds and loads stream data at the specified start and and finish time 
        Args:
            startTime (UTC_DateTime): the desired start time of the stream
            endTime (UTC_DateTime): the desired end time of the stream

        Returns:
            _type_: _description_
        """                
        startFile = self.findFile_time(startTime-10,component)
        if not startFile:
            print("noFile!")
            return None
        endFile = self.findFile_time(endTime+10,component)            
        stream = self.loadStream_file(startFile)
        if startFile != endFile:
            print("double file!")
            stream = stream.extend(self.loadStream_file(endFile)).merge()
        stream = stream.trim(startTime,endTime)
        if len(stream) == 0:
            print("stop!")
        return stream
    
    def streamGen(self, sr = 100):
        """generates a stream object that 

        Args:
            sr (_type_): _description_
            times (_type_): _description_
        """
        stopFile = self.findFile_time(self.recoveryTime)
        for fileTime in self.fileTimes:
            file = fileTime[1]
            files = "*.".join(file.split('.'))
            time = fileTime[0]                        
            stream = self.loadStream_file(files) 
            stream.decimate(10)
            replacements = {'Z':'Z','X':'N', 'Y':'E'}
            for tr in stream:
                code = tr.stats.channel
                newAx = replacements[code[-1]]
                tr.stats.channel = code[:-1] + newAx                            
            if file == stopFile:
                return stream.trim(endtime = self.recoveryTime)
            else:
                yield stream
            
    def adjAtt(self,inv):
        self.stream._rotate_to_zne(inv,"ZXY")            

    def axisTo_ZNE(self, ):
        """
        replace XYZ channels with  Z, N, E channels
        Z gets replaced
        would just add new ones but its too slow
        
        """

        replacements = {'Z':'Z','X':'N', 'Y':'E'}
        dips = {'Z':-90,'N':'0','E':0}
        azimuths = {'Z':0,'N':0,'E':90}
        for c in self:                        
            newAx = replacements[c.code[-1]]
            c.code = c.code[:-1] + newAx
            #c.dip = dips[newAx]
            #c.azimuth = azimuths[newAx]
               
    def loadStream_pick(self,pick, window = 20):
        """A function to load stream time for a given phasenet pick

        Args:
        pick
        window (int): seconds

        """
        startTime = UTCDateTime(pick.phase_time) - timedelta(seconds = window/2)    
        endTime = UTCDateTime(pick.phase_time) +timedelta(seconds = window/2)
        pickStream = self.loadStream_time(startTime,endTime)
        pickStream.phase_index = pick.phase_index
        return pickStream
     
    def genStreams_picks(self, picks, window = 20):
        """efficiently loads and lists every stream for every pick in a dataframe 
           also adds phase_index to stream for reference

        Args:
            picks (DataFrame): a pandas dataframe describing picks, must have phase_time for each pick  
            window (int, optional): _description_. specify region of time around pick to load
        """
        
        picks['file'] = picks['phase_time'].apply(lambda x: self.findFile_time(UTCDateTime(x)))
        pGroups = picks.groupby('file')['phase_time'].apply(list)
        picks_ti = picks.set_index("phase_time")#to find other values with 
        streams = []
        for file,times in tqdm(pGroups.items()):
            stream = self.loadStream_file(file)
            print(stream)            
            for time in times:                
                cut = stream.slice(UTCDateTime(time)-timedelta(seconds = window/2), UTCDateTime(time) + timedelta(seconds = window/2))
                cut.phase_index = picks_ti.loc[time].phase_index
                if len(cut) ==0:
                    print(stream)
                    print(time)
                    print(file)
                yield(cut)
            
    def setFR(self, test):        
        for c in self.channels:
            c.setFR(test)
            
    def predictPrecision(n_days =35):
        nd_precKey = '{}day_mPrecision'.format(n_days)
        nd_precKey_d = '{}day_degPrecision'.format(n_days)
        d_meas = (max(times[start:])-min(times[start:])).total_seconds()/86400
        for key in pos_floats:
            datStats[key][nd_precKey] = datStats[key]['m_err']*np.sqrt(d_meas/n_days) #35 day error
        for key in angle_floats:
            datStats[key][nd_precKey_d] = datStats[key]['mean_err']*np.sqrt(d_meas/n_days) #35 day error
        ####params for both plots
        fig,ax = plt.subplots(2)
        barWidth = 0.3
        #####plot position precision
        br1 = np.arange(len(pos_floats))
        heights0 = [datStats[key]['std']/5 for key in pos_floats]
        heights1 = [datStats[key]['m_err'] for key in pos_floats]
        heights2 = [datStats[key][nd_precKey] for key in pos_floats]
        ax[0].bar(br1, heights0, color ='r', width = barWidth, edgecolor ='grey', label ='standard deviation/5')
        ax[0].bar(br1+barWidth, heights1, color ='b', width = barWidth, edgecolor ='grey', label ='dataset precision')
        ax[0].bar(br1+2*barWidth, heights2, color ='g', width = barWidth, edgecolor ='grey', label =("{} day precision").format(n_days))
        ax[0].set_ylabel("meters")
        ax[0].set_xticks(br1 + barWidth, pos_floats)
        ###########for degrees
        br1 = np.arange(len(angle_floats))
        heights0 = [datStats[key]['std']/5 for key in angle_floats]
        heights1 = [datStats[key]['mean_err'] for key in angle_floats]
        heights2 = [datStats[key][nd_precKey_d] for key in angle_floats]
        ax[1].bar(br1, heights0, color ='r', width = barWidth, edgecolor ='grey', label ='standard deviation/5')
        ax[1].bar(br1+barWidth, heights1, color ='b', width = barWidth, edgecolor ='grey', label ='dataset precision')
        ax[1].bar(br1+2*barWidth, heights2, color ='g', width = barWidth, edgecolor ='grey', label =("{} day precision").format(n_days))
        ax[1].set_ylabel("degrees")
        ax[1].set_xticks(br1 + barWidth, angle_floats)
        ax[1].bar
        plt.legend()
        plt.show()

    def writeStream(self, stream):
        outDir = self.dir+'_'     
        if not os.path.exists(outDir):
            print("eh")
            os.mkdir(outDir)          
        fname = outDir+"/stream{}.MiniSeed".format(str(stream[0].stats.starttime))
        stream.write(fname,format = "MSEED")
        return fname

class SS_Channel(Channel):
    """This represents an x,y, or z component of a 3c smartsolo device

    Args:
        Channel (_type_): _description_
    """

    def __init__(self, station, frTest,  axes, dip, azimuth, gain):
        self.TD = 3.31009*10**(-6) #default ,for transfer function
        self.KH=-8.66463*10**-6 # default, for transfer function
        #self.Dd = 0.70722 #default damping from manual
       # self.Gd = 78.54688#default sensitivity from manual
        self.gain = gain        
        self.axes = axes
        self.station = station
        self.aaFilter = self.station.anti_alias_filter_type
        super().__init__(                         
        "LH"+axes,
        location_code = "",
        longitude = station.longitude,
        latitude = station.latitude,
        dip = dip,
        azimuth = azimuth%360,
        elevation = station.elevation,
        sample_rate = station.sample_rate,
        #location = "({},{})".format(station.latitude, station.longitude),
        depth = 0)
        self.setFR(frTest)

    def setFR_nrl():
        """outdated"""
        manufacturer = 'DTCC (manufacturers of SmartSolo'
        device = 'SmartSolo IGU-16HR3C'
        preampGain = list(filter(lambda key: ("{} dB".format(gain) in key),NRL().dataloggers[manufacturer][device]))## match preamp gain from NRL
        filterType = 'Linear Phase'
        IIR_Low_Cut = 'Off'
        low_freq = '5 Hz'
        sensorKeys = ['DTCC (manuafacturers of SmartSolo)','5 Hz','Rc=1850', 'Rs=430000']
        dataloggerKeys = [manufacturer, device, preampGain, sampleRate, filterType, IIR_Low_Cut]
        nrl = NRL()
        response = nrl.get_response(
            sensor_keys=sensorKeys,
            datalogger_keys=dataloggerKeys)
        response.plot(min_freq=1E-4, output='DISP')

    def setFR(self, test):        
        """writes frequency response data to the channel. reads device info from the dir, 
        and calculates poles of the frequency response
        Args:
            dir (string): the file should contain calibration data from the smartsolo software with the test rack. 

        Raises:
            Exception: 

        Returns:
            _type_: 
        """        
        if test is not None:
            sensTest = test[self.axes + " S.Sensitivity.(V/m/s)"]  
            self.sensitivity = 1000*sensTest*10**(0.05*self.gain)#adjust with gain, also mV to V           
            self.damping = test[self.axes + " S.Damping"]
            self.w0 = test[self.axes + " S.Frequency(Hz)"]*2*np.pi            
            self.response = Response.from_paz([0,0],self.getPoles()[1:], self.sensitivity)        
        
    def getPoles(self):
        """Assumes the transfer function from SmartSolo, 
        H(s) = s^2/((1+T_D*s)*(s^2+2*T_D*D_d*w_0*s+w_0^2)+KH*s^2   
        default values from manual
        and returns the poles of the transfer function
        Args:
            TD (_type_, optional): _description_. Defaults to 3.31009*10**(-6)
            KH (_type_, optional): _description_. Defaults to -8.66463*10**-6.
            w0 (float, optional): the natural frequency, Defaults to ten pi, 5hz
            Dd (float, optional): Damping. Defaults to 0.70722.
        """
        TD = self.TD        
        Dd = self.damping
        w0 = self.w0
        p3 = TD
        p2 = 1+self.KH+2*TD*Dd*w0
        p1 = TD*(w0**2)+2*Dd*w0
        p0 = w0**2
        return np.roots([p3,p2,p1,p0])

def plot_rotated_axes(ax, r, name=None, offset=(0, 0, 0), scale=1):
    colors = ("#FF6666", "#005533", "#1199EE")  # Colorblind-safe RGB
    loc = np.array([offset, offset])
    for i, (axis, c) in enumerate(zip((ax.xaxis, ax.yaxis, ax.zaxis),colors)):                                      
        axlabel = axis.axis_name
        axis.set_label_text(axlabel)
        axis.label.set_color(c)
        axis.line.set_color(c)
        axis.set_tick_params(colors=c)
        line = np.zeros((2, 3))
        line[1, i] = scale
        line_rot = r.apply(line)
        line_plot = line_rot + loc
        ax.plot(line_plot[:, 0], line_plot[:, 1], line_plot[:, 2], c)
        text_loc = line[1]*1.2
        text_loc_rot = r.apply(text_loc)
        text_plot = text_loc_rot + loc[0]
        ax.text(*text_plot, axlabel.upper(), color=c,
                va="center", ha="center")
        ax.text(*offset, name, color="k", va="center", ha="center",
            bbox={"fc": "w", "alpha": 0.8, "boxstyle": "circle"})
        










#startTime = UTCDateTime(2023,10,13,16,8,47,1)-timedelta(seconds = 0)
#endTime = UTCDateTime(2023,10,13,17,47,1)+timedelta(seconds = 000)
#if __name__ == "__main__":
if False:
    inv = SS_inventory("SS_inventory")
    net = inv[0]
    net.SSarray.py()
    for s in net.stations:
        for x in tqdm(range(s.n_files)):
            s.loadStream_number(x)     
            s.adjAtt(inv)
            net.write()
            replacements = {'Z':'Z','N':'X', 'E':'Y'}
            for c in s:
                c.code = c.code[:2] +replacements[c.code[2]]
        print(s.start_date)
    df = net.plotAgainstTemp()
    tilt_temp = (df.set_index('temperature')['ecompass_north'])
    
    
    net.plot_response(0.1)
    net.plotLogNumericals()
    net.stations = net.stations[:1]
    net.plotLogNumericals()
    s1 = net.stations[2]
    st = s.loadStream(8)
    st1 = s1.loadStream(4)
    from datetime import timedelta
    t_start = st[0].stats.endtime-timedelta(0,24000)
    t_start2 = st[0].stats.endtime-timedelta(0,10000)
    t_stop = st[0].stats.endtime-timedelta(0,5000)
    st.trim(t_start,t_stop)




